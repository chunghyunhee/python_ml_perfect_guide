{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ch 08. 텍스트 분석\n",
    "\n",
    "1) NLP (Natural Language Processing)\n",
    "- 머신이 인간의 언어를 이해/해석하는데 관심\n",
    "- 자연(언)어: 사람들이 일상적으로 쓰는 언어를 인공적으로 만들어진 언어인 '인공어'와 구분해서 부르는 개념\n",
    "(인공어: 사람의 의도/목적에 따라 만든 언어)\n",
    "\n",
    "2) 텍스트 분석 (Text Analytics, TA) (=텍스트 마이닝)\n",
    "- 비정형 텍스트에서 의미있는 정보를 추출하는데 관심\n",
    "- 머신러닝, 언어이해, 통계 등을 활용하여 모델을 만들고 정보를 추출해 비즈니스 인텔리전스나 예측 등 분석작업을 수행\n",
    "\n",
    "-> 위의 1)과 2)는 모두 머신러닝에 기반함\n",
    "\n",
    "- 텍스트분석의 주요 기술영역:\n",
    "1) 텍스트 분류 (Text Classification, Text Categorization)\n",
    ": 문서가 특정 분류나 카테고리에 속하는 것을 예측 (지도학습 활용)\n",
    "2) 감정 분석 (Sentiment Analysis)\n",
    ": (소셜 미디어, 리뷰, 여론조사 등에서의) 감정 등 주관적 요소를 분석 (지도 or 비지도학습 활용)\n",
    "3) 텍스트 요약 (Summarization)\n",
    ": 중요한 주제나 중심 사상을 추출 (ex. 토픽 모델링)\n",
    "4) 텍스트 군집화 (Clustering) 및 유사도 측정\n",
    ": 비슷한 유형의 문서에 대해 군집화 수행 (비지도학습 활용)\n",
    ": 유사도 측정은 문서들간의 유사도를 측정\n",
    "\n",
    "<01. 텍스트 분석 이해>\n",
    "- 비정형데이터인 텍스트를 분석\n",
    "- 머신러닝에 적용하기 위해서는, 피처벡터화 (또는 피처추출 (즉, 텍스트를 벡터값을 가지는 피처로 변환)이 우선 필요 (for 머신러닝 모델적용)\n",
    "(즉, 텍스트를 단어 (또는 단어의 일부분) 기반의 다수의 피처로 추출하고, 이 피처에 단어 빈도수와 같은 숫자값을 부여하면\n",
    "텍스트는 단어의 조합인 벡터값으로 표현됨)\n",
    "- 피처벡터화 방법: BOW (Bag Of Words), Word2Vec 방법\n",
    "\n",
    "[텍스트 분석 수행 과정]\n",
    "1. 텍스트 사전 준비작업 (=텍스트 전처리)\n",
    ": 1) 클렌징 작업 (대/소문자 변경, 특수문자 삭제 등)\n",
    "2) 단어 등의 토큰화 작업\n",
    "3) 의미없는 단어 (Stop word) 제거 작업\n",
    "4) 어근 추출 (Stemming/Lemmatization) 등의 텍스트 정규화 작업\n",
    "\n",
    "2. 피처 벡터화/추출\n",
    ": 사전 가공된 텍스트에서, 피처를 추출하고, 벡터값을 할당\n",
    "- 방법: 1) BOW (=Count 기반, TF-IDF 기반의 벡터화), 2) Word2Vec\n",
    "\n",
    "3. ML모델 수립, 학습/예측/평가\n",
    ": 피처 백터화된 데이터셋에 ML모델 적용\n",
    "\n",
    "[파이썬 기반의 NLP, 텍스트 분석 패키지]\n",
    "- 주요 3가지 NLP와 텍스트분석 패키지:\n",
    "1) NLTK (: NLP의 거의 모든 영역을 커버하나, 속도가 느림)\n",
    "2) Gensim (: 토픽 모델링)\n",
    "3) SpaCy (: 가장 핫한 최신 패키지)\n",
    "\n",
    "- 사이킷런은 NLP 패키지에 특화된 라이브러리가 없음\n",
    "\n",
    "<02. 텍스트 사전 준비작업 (텍스트 전처리): 텍스트 정규화>\n",
    "\n",
    "- 텍스트 정규화 (NLTK 패키지 활용)\n",
    ": 클렌징, 정제, 토큰화, 어근화 등의 사전작업\n",
    "(즉, Cleansing, Tokenization, 필터링/스톱워드제거/철자수정, Stemming, Lemmatization)\n",
    "\n",
    "(1) 클렌징\n",
    ": 분석에 방해되는 불필요한 문자, 기호 등을 사전에 제거\n",
    "(ex. HTML, XML 태그나 특정기호)\n",
    "\n",
    "(2) 텍스트 토큰화\n",
    ": 문장 토큰화 (문서에서 문장을 분리), 단어 토큰화 (문장에서 단어를 토큰으로 분리)\n",
    "\n",
    "1) 문장 토큰화 (Sentence Tokenization)\n",
    ": 문장의 마지막에 쓰는 기호 (마침표(.), 개행문자(\\n) 등)에 따라 분리\n",
    ": NTLK에서 sent_tokenize 이용\n",
    ": 아래 코드 실행 이전에 pip install nltk으로 먼저 설치하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk #맨 위 2줄 먼저 실행해야 아래 코드 돌아감\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room.  \\\n",
    "              You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences=sent_tokenize(text=text_sample)\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": 위 결과가 반환한 것은, 각 문장으로 구성된 리스트 객체다. 반환된 리스트 객체는 3개의 문장으로 된 문자열을 가진다.\n",
    "\n",
    "2) 단어 토큰화 (Word Tokenization)\n",
    ": 문장을 단어로 토큰화\n",
    ": Bag of Words와 같이 단어 순서가 안 중요한 경우, 문장 토큰화 안 하고, 바로 단어 토큰화만 사용하면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words=word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<위 2가지 결합하기>\n",
    ": 문서 -> 문장 -> 단어로 토큰화\n",
    ": tokenize_text() 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    sentences=sent_tokenize(text) #문장별로 분리 토큰\n",
    "    \n",
    "    word_tokens=[word_tokenize(sentence) for sentence in sentences] #분리된 문장별 단어 토큰화\n",
    "    \n",
    "    return word_tokens\n",
    "\n",
    "word_tokens=tokenize_text(text_sample)\n",
    "\n",
    "print(type(word_tokens),len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": 1) word_tokens는 3개의 리스트 객체를 내포하는 리스트다.\n",
    "그리고 내포된 개별 리스트 객체는 각각 문장별로 토큰화된 단어를 요소로 가진다. (2)\n",
    "\n",
    "- 위 방법의 한계는 문맥적 의미가 무시된다는 점.\n",
    "- 해결방법: n-gram 도입\n",
    "(n-gram은 연속된 n개 단어를 하나의 토큰화 단위로 분리해내는 것이다.\n",
    "n개 단어 크기 윈도우를 만들어, 문장의 처음부터 오른쪽으로 움직이면서 토큰화를 수행한다.\n",
    "ex. \"agent smith knocks\"를 2-gram (bigram)으로 만들면,\n",
    "(agent, smith), (smith, knocks)와 같이 연속적으로 2개의 단어들을 순차적으로 이동하면서 단어들을 토큰화한다.\n",
    "\n",
    "<스톱워드 제거>\n",
    "- 스톱 워드: 분석에 별 의미가 없는 단어 (ex. is, the, a, will)\n",
    "- 제거 안 하면, 오히려 중요한 단어로 인식 (빈도가 높아서)하므로, 전처리 작업으로 필수적\n",
    "- NLTK은 가장 다양한 언어들에 대한 스톱워드 목록을 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "print('영어 stop words 개수:', len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20]) #20개만 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<이전 예제 데이터로부터 스톱워드 제거하기>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "all_tokens=[]\n",
    "\n",
    "#위 예제에서 3개 문장별로 얻은 word_tokens 리스트에 대해 스톱워드 제거하는 반복문\n",
    "for sentence in word_tokens:\n",
    "    filtered_words=[]\n",
    "    \n",
    "    #개별 문장별 토큰화된 문장 리스트에 대해, 스톱워드 제거하는 반복문\n",
    "    for word in sentence:\n",
    "        word=word.lower() #소문자로 모두 변환\n",
    "        \n",
    "        if word not in stopwords: #if (토큰화된 개별단어) not in (스톱워드의 단어): (즉, word가 stopwords에 없으면 word_tokens에 추가) \n",
    "            filtered_words.append(word)\n",
    "            \n",
    "    all_tokens.append(filtered_words)\n",
    "    \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": is, this 등 스톱워드들이 제거되었다.\n",
    "\n",
    "<Stemming, Lemmatization>\n",
    "- 문법에 따라 같은 단어가 여러 형태로 바뀌어 있는데,\n",
    "이들은 문법 또는 의미적으로 변하는 단어의 원형을 찾아준다.\n",
    "- 둘은 비슷하나, 후자가 더욱 정확하게 어근단어를 찾아줌 (but 수행 속도가 느림)\n",
    "\n",
    "(1) Stemming\n",
    "- NLTK의 Porter, Lancaster, Snowball Stemmer\n",
    "\n",
    "(2) Lemmatization\n",
    "- NLTK의 WordNetLemmatizer\n",
    "\n",
    "- 실습하면서 비교하기:\n",
    "\n",
    "<1> NLTK: LancasterStemmer 활용하기\n",
    "- LancasterStemmer()으로 Stemmer 객체를 생성한 후, 객체의 stem('단어') 메서드를 호출하면, 원하는 단어의 Stemming이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "happy happy happiest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer=LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked')) #정확한 결과 출력\n",
    "print(stemmer.stem('happy'), stemmer.stem('happier'), stemmer.stem('happiest')) #잘못된 결과 출력 (한계)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<2> NLTK: WordNetLemmatizer 활용하기\n",
    "- 보다 정확하고자 원형 단어의 '품사'를 입력해줘야\n",
    "- 즉, lemmatize()의 모수로서, 1) 동사는 'v', 2) 형용사는 'a'를 입력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy happy\n"
     ]
    }
   ],
   "source": [
    "import nltk #맨 위 2줄 먼저 실행해야 아래 코드 돌아감\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "print(lemma.lemmatize('amusing','v'), lemma.lemmatize('amuses','v'), lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happy','a'), lemma.lemmatize('happier','a'), lemma.lemmatize('happiest','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<03. Bag of Words - BOW>\n",
    "- 문서의 모든 단어들의 문맥, 순서를 무시하고 빈도값 (word count 기반)을 세어 피처 값을 추출\n",
    "- 문서 내 모든 단어를 한꺼번에 봉투(백) 안에 넣은 뒤, 흔들어 섞는다는 의미로 Bag of Words 모델\n",
    "\n",
    "- 장점: 쉽고 빠른 구축, 문서의 특징을 잘 나타낼 수 있는 모델\n",
    "- 단점: \n",
    "1) 문맥 의미 (Semantic context) 반영이 부족 (: 순서 무시하므로, 단어의 문맥적 의미가 무시됨, 보완하고자 n-gram 기법이 도입되었지만 제한적인 보완 가능)\n",
    "2) 희소행렬 (Sparse Matrix, 대부분의 행렬 값이 0으로 채워지는) 문제 존재 (: ML알고리즘의 수행시간, 예측성능을 저하시킴)\n",
    "\n",
    "[BOW 피처 백터화]\n",
    "- 머신러닝 알고리즘은 숫자형 피처를 데이터로 입력받아 수행됨\n",
    "- 텍스트는 바로 입력하지 못하므로, 텍스트를 숫자형 값인 벡터값으로 변환 (즉, 피처 벡터화)하기\n",
    "- 각 문서 (Document)의 텍스트를 단어로 추출해 피처로 할당하고, 각 단어의 발생빈도 값을 피처에 값으로 부여해, 각 문서를 단어피처의 발생빈도값으로 구성된 벡터로 만듬\n",
    "cf. 텍스트 분석에서는 '피처 백터화' = '피처 추출'\n",
    "- 피처 백터화: 모든 문서에서 모든 단어를 열 형태로 나열하고, 각 문서에서 해당 단어의 횟수나 정규화된 빈도를 값으로 부여하는 데이터셋 모형으로 변경하는 과정\n",
    "- 예) M개의 텍스트 문서 (모든 문서들=Corpus)가 있고, 이 문서에서 모든 단어를 추출해 나열 시, N개의 단어가 (각 문서마다 동일하게) 존재한다면, 문서의 피처백터화를 해보면, M개의 문서는 각각 N개의 값이 할당된 피처의 백터세트가 된다.즉, (M,N)차원의 행렬이 된다.\n",
    "\n",
    "- 2가지 방법:\n",
    "1) 카운트 (기반의) 벡터화\n",
    ": 카운트 값이 높을수록 무조건 중요단어로 인식 (한계점 존재)\n",
    "\n",
    "2) TF-IDF (Term Frequency - Inverse Document Frequency) (기반의) 벡터화\n",
    ": 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서, 패널티를 부여\n",
    "ex. '많은', '빈번하게', '당연히', '조직', '업무' 등의 단어는 문서의 특징과는 관련이 적으나, 보편적으로 자주 사용되므로 문서에 자주 나타난다.\n",
    "-> 이런 단어들에 패널티를 부여하여 가중치의 균형을 맞추는 것이다.\n",
    "- 텍스트 길이가 길수록, 문서의 개수가 많을수록, TF-IDF 방식을 써야 더 좋은 예측성능이 나온다.\n",
    "\n",
    "[사이킷런의 카운트 및 TF-IDF 벡터화 구현하기: CountVectorizer, TfidfVectorizer]\n",
    "- 사이킷런의 CounterVectorizer 클래스 <- 카운트 기반의 벡터화\n",
    ": 이 클래스는 피처 백터화 (& 소문자 일괄 변환, 토큰화, 스톱워드 필터링 등의 텍스트 전처리) 함께 수행\n",
    "- fit(), transform() 써서 피처 백터화된 객체를 반환\n",
    "\n",
    "- 입력 파라미터:\n",
    "1) max_df\n",
    ": 전체 문서에서 너무 높은 빈도수를 갖는 단어 피처 (스톱워드와 비슷한 문법적 특성의한 반복단어일 가능성이 높음) 제외시킴\n",
    ": max_df=100 (전체 문서에 걸쳐 100개 이하로 나타나는 단어만 피처로 추출)\n",
    "또는 max_df=0.95 (빈도수의 95% 이하인 단어까지만 피처로 추출, 상위 5%는 피처로 추출하지 않음)\n",
    "\n",
    "2) min_df\n",
    ": 전체 문서에서 너무 낮은 빈도수를 갖는 단어 피처 (중요성이 낮거나, 쓰레기 단어로 간주) 제외시킴\n",
    ": min_df=2 (2번 이하로 나타난 단어는 피처로 추출 안 함)\n",
    "또는 min_df=0.02 (하위 2% 이하의 빈도수를 갖는 단어는 피처로 추출 안 함)\n",
    "\n",
    "3) max_features\n",
    ": 추출하는 피처의 개수를 제한 (정수 값만 가능)\n",
    ": max_features=2000 (가장 높은 빈도를 가지는 단어 순으로 정렬하여, 2000개까지만 피처로 추출함)\n",
    "\n",
    "4) stop_words='english'\n",
    ": 영어의 스톱워드로 지정된 단어는 추출 안 함 (영어만 이 기능 사용가능)\n",
    "\n",
    "5) n_gram_range\n",
    ": Bag of words 모델의 단어순서를 보완/고려하기 위한 n_gram 범위를 설정\n",
    ": 튜플 형태로 (범위 최소값, 범위 최대값)을 지정\n",
    ": (1,1) -> 토큰화된 단어를 1개씩 피처로 추출\n",
    ": (1,2) -> 토큰화된 단어를 1개씩 (minimum 1), 그리고 순서대로 2개씩 (maximum 2) 묶어서 피처로 추출\n",
    "\n",
    "6) 기타 생략\n",
    "\n",
    "- CountVectorizer를 이용한 피처 백터화 과정:\n",
    "(1) 영어의 경우, 모든 문자를 소문자로 변경하는 등의 전처리 작업\n",
    "(2) 디폴트로 단어 기준으로 n_gram_range를 반영해 각 단어를 토큰화\n",
    "(3) 텍스트 정규화 (1. 토큰화 (Tokenization), 2. 어간추출 (Stemming)) 수행\n",
    "cf. Stemmer, Lemmatize는 CountVectorizer 자체에서 지원되지 않으므로, 함수를 따로 만들거나 외부 패키지로 미리 Text normalization 수행하기\n",
    "(4) 피처 백터화 하기 (: max_df, min_df, max_features 등을 활용해 토큰화된 단어를 피처로 추출하고, 단어 빈도수 벡터값을 적용)\n",
    "\n",
    "- 두번째 방법 (TF-IDF 벡터화)는 TfidfVectorizer 클래스를 이용하는데, 파라미터와 변환방법은 위 첫번째 방법과 동일!\n",
    "\n",
    "[BOW 벡터화를 위한 희소행렬]\n",
    "- 사이킷런 CountVectorizer, TfidfVectorizer: 텍스트 -> 1) 피처 단위로 벡터화 변환, 2) CSR형태의 희소행렬 반환\n",
    "- BOW 형태를 가진 언어모형의 피처 백터화는 대부분 희소행렬\n",
    "- 희소행렬은 메모리, 시간을 많이 낭비하므로, 변환이 필요: 방법 2가지 = 1) COO 형식, 2) CSR 형식 (더 많이 사용)\n",
    "\n",
    "1) 희소행렬: COO (Coordinate, 좌표) 형식\n",
    "- 0이 아닌 데이터만 별도의 배열(array)에 저장, 그 행렬 위치를 별도의 배열에 저장\n",
    "- 예) 2차원 데이터: [[3,0,1],[0,2,0]]\n",
    "-> 0이 아닌 데이터는 [3,1,2]\n",
    "-> 이들의 위치는 (0,0), (0,2), (1,1)\n",
    "-> 세 값의 위치를 행끼리, 열끼리 모으면: Row는 [0,0,1], Col은 [0,2,1]\n",
    "\n",
    "- 희소행렬 변환: 사이파이(Scipy)의 sparse 패키지 이용\n",
    "(먼저 데이터를 넘파이의 ndarray 객체로 만든 후, COO 형식의 희소행렬로 변환시킴)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dense=np.array([[3,0,1],[0,2,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 밀집행렬을 사이파이의 coo_matrix 클래스를 써서, COO 형식의 희소행렬로 변환하기\n",
    ": 0이 아닌 데이터를 별도의 배열 데이터로 만들고,\n",
    "행 위치 배열과 열 위치 배열을 각각 만든 뒤,\n",
    "coo_matrix() 내 생성 모수로 입력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-592965061c51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'scipy'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#왜 오류날까?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sparse'"
     ]
    }
   ],
   "source": [
    "import sparse\n",
    "sparse.download('scipy') #왜 안되?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#미리 아나콘다 프롬프트에서 pip install sparse 설치하기\n",
    "from scipy import sparse #(sparse) 패키지 이용\n",
    "\n",
    "data=np.array([3,1,2])\n",
    "\n",
    "row_pos=np.array([0,0,1])\n",
    "col_pos=np.array([0,2,1])\n",
    "\n",
    "sparse_coo=sparse.coo_matrix((data, (row_pos, col_pos))) #(.coo_matrix)로 희소행렬 생성\n",
    "#sparse_coo: COO형식의 희소행렬 객체변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coo.toarray() #다시 밀집(dense) 형태의 행렬로 출력시키기 (원래의 데이터행렬로 추출됨)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 희소행렬: CSR (Compressed Sparsed Row) 형식 (1번 방법보다 개선됨)\n",
    "- 인덱스의 인덱스 (위치의 위치)를 표기, 중복 제거, 배열의 맨 마지막에 데이터의 총 개수를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "             [1,4,0,3,2,5],\n",
    "             [0,6,0,3,0,0],\n",
    "             [2,0,0,0,0,0],\n",
    "             [0,0,0,7,0,8],\n",
    "             [1,0,0,0,0,0]])\n",
    "\n",
    "# 0 이 아닌 데이터 추출\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성 \n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "# COO 형식으로 변환 \n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos,col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
    "\n",
    "# CSR 형식으로 변환 \n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray())\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "print(sparse_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense3 = np.array([[0,0,1,0,0,5],\n",
    "             [1,4,0,3,2,5],\n",
    "             [0,6,0,3,0,0],\n",
    "             [2,0,0,0,0,0],\n",
    "             [0,0,0,7,0,8],\n",
    "             [1,0,0,0,0,0]])\n",
    "\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.csr_matrix(dense3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<04. 텍스트 분류 예제: 20 뉴스그룹 분류하기>\n",
    "- 텍스트 분류\n",
    ": 특정 문서의 분류를, 학습데이터를 통해 학습하여 모델을 생성한 후, 이 모델을 이용해 새로운 다른 문서의 분류를 예측한다.\n",
    "- 텍스트를 피처 백터화로 변환하면, 희소행렬 형태가 된다.\n",
    "- 위 희소행렬에 효과적인 알고리즘 3가지: 1) 로지스틱 회귀, 2) 선형 서포트 벡터 머신, 3) 나이브 베이즈 등\n",
    "- 텍스트 기반으로 분류 시, 1) 텍스트 정규화 -> 2) 피처 백터화 -> 3) 적합한 머신러닝 알고리즘을 적용해, 분류를 학습/예측/평가\n",
    "\n",
    "[텍스트 정규화]\n",
    "- 사이킷런 예제 데이터 (API): fetch_20newsgroups() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups #이건 pip install 등 미리 안 해도 바로 다운로드 됨\n",
    "news_data=fetch_20newsgroups(subset='all',random_state=156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "print(news_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 클래스의 값과 분포도 \n",
      " 0     799\n",
      "1     973\n",
      "2     985\n",
      "3     982\n",
      "4     963\n",
      "5     988\n",
      "6     975\n",
      "7     990\n",
      "8     996\n",
      "9     994\n",
      "10    999\n",
      "11    991\n",
      "12    984\n",
      "13    990\n",
      "14    987\n",
      "15    997\n",
      "16    910\n",
      "17    940\n",
      "18    775\n",
      "19    628\n",
      "dtype: int64\n",
      "target 클래스의 이름들 \n",
      " ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print('target 클래스의 값과 분포도 \\n', pd.Series(news_data.target).value_counts().sort_index())\n",
    "print('target 클래스의 이름들 \\n', news_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\n",
      "Subject: Re: Observation re: helmets\n",
      "Organization: Sun Microsystems, RTP, NC\n",
      "Lines: 21\n",
      "Distribution: world\n",
      "Reply-To: egreen@east.sun.com\n",
      "NNTP-Posting-Host: laser.east.sun.com\n",
      "\n",
      "In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n",
      "> \n",
      "> The question for the day is re: passenger helmets, if you don't know for \n",
      ">certain who's gonna ride with you (like say you meet them at a .... church \n",
      ">meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n",
      ">pick up another shoei in my size to have a backup helmet (XL), or should I \n",
      ">maybe get an inexpensive one of a smaller size to accomodate my likely \n",
      ">passenger? \n",
      "\n",
      "If your primary concern is protecting the passenger in the event of a\n",
      "crash, have him or her fitted for a helmet that is their size.  If your\n",
      "primary concern is complying with stupid helmet laws, carry a real big\n",
      "spare (you can put a big or small head in a big helmet, but not in a\n",
      "small one).\n",
      "\n",
      "---\n",
      "Ed Green, former Ninjaite |I was drinking last night with a biker,\n",
      "  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\n",
      "DoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n",
      " (The Grateful Dead) -->  |It seemed like the least I could do...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": 위는 데이터를 한개만 추출해 값을 확인한 것이다.\n",
    "\n",
    "- 제목 등 다른 정보는, target 클래스 값과 유사한 데이터므로 삭제해야 한다.\n",
    "- 순수한 텍스트 (내용 본문)만으로 구성된 기사 내용에 근거하여, 개별 데이터가 어떤 뉴스그룹에 속하는지 분류해야 된다.\n",
    "1) remove 모수 -> 뉴스그룹 기사의 헤더, 푸터(footer) 등을 제거\n",
    "2) subset 모수 -> 학습/테스트 데이터셋을 분리해 data 다운로드 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터 크기 11314, 테스트데이터 크기 7532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train_news=fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'),random_state=156)\n",
    "X_train=train_news.data\n",
    "y_train=train_news.target\n",
    "\n",
    "test_news=fetch_20newsgroups(subset='test',remove=('headers','footers','quotes'),random_state=156)\n",
    "\n",
    "X_test=test_news.data\n",
    "y_test=test_news.target\n",
    "\n",
    "print('학습데이터 크기 {0}, 테스트데이터 크기 {1}'.format(len(train_news.data), len(test_news.data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[피처 백터화 변환, 머신러닝 모델 학습/예측/평가]\n",
    "\n",
    "- 데이터:\n",
    "1) 학습 데이터: 11,314개 뉴스그룹 문서 (리스트)\n",
    "-> CountVectorizer로 피처 백터화\n",
    "2) 테스트 데이터: 7,532개 문서 (리스트)\n",
    "-> (주의) 반드시 학습 데이터를 써서 fit()이 수행된 CountVectorizer 객체를 이용해 테스트 데이터를 변환하기\n",
    "즉, 학습 데이터에 사용된 CountVectorizer 객체변수인 cnt_vect.transform()를 써서 변환하기\n",
    "(즉, fit_transform()을 쓰면 안 됨 <- 이유: 학습에 사용된 피처 개수와, 예측에 사용할 피처 개수가 달라짐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터 텍스트의 CountVectorizer Shape: (11314, 101631)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cnt_vect=CountVectorizer()\n",
    "cnt_vect.fit(X_train, y_train)\n",
    "X_train_cnt_vect=cnt_vect.transform(X_train)\n",
    "\n",
    "X_test_cnt_vect=cnt_vect.transform(X_test) #학습데이터로 fit()된 CountVectorizer 써서, 테스트 데이터를 피처백터화 변환 수행\n",
    "print('학습데이터 텍스트의 CountVectorizer Shape:',X_train_cnt_vect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": 학습데이터를 CountVectorizer로 피처를 추출한 결과, 11,314개 문서에서 피처 (즉, 단어)가 101,631개 만들어짐\n",
    "\n",
    "- 이제, 피처 백터화된 데이터에 로지스틱회귀 적용하여, 뉴스그룹에 대한 분류를 예측하기:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jiwon\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Jiwon\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorized Logistic Regression의 예측 정확도는 0.617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr_clf=LogisticRegression()\n",
    "lr_clf.fit(X_train_cnt_vect,y_train)\n",
    "pred=lr_clf.predict(X_test_cnt_vect)\n",
    "print('CountVectorized Logistic Regression의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": 위까지는 Count 기반 벡터화였다.\n",
    "\n",
    "- 아래부터는 TF-IDF 기반 벡터화로 변경하여 예측 모형을 수행하기:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Logistic Reg의 예측정확도는 0.678\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect=TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect=tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect=tfidf_vect.transform(X_test)\n",
    "\n",
    "lr_clf=LogisticRegression()\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "pred=lr_clf.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Logistic Reg의 예측정확도는 {0:.3f}'.format(accuracy_score(y_test,pred)))\n",
    "#이전 방법보다 훨씬 높은 예측 정확도를 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorized Logistic Reg의 예측정확도는 0.690\n"
     ]
    }
   ],
   "source": [
    "#이제, TF-IDF에서 모수 설정을 디폴트 외에 다르게 해보자:\n",
    "\n",
    "tfidf_vect=TfidfVectorizer(stop_words='english',ngram_range=(1,2),max_df=300)\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect=tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect=tfidf_vect.transform(X_test)\n",
    "\n",
    "lr_clf=LogisticRegression()\n",
    "lr_clf.fit(X_train_tfidf_vect,y_train)\n",
    "pred=lr_clf.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Vectorized Logistic Reg의 예측정확도는 {0:.3f}'.format(accuracy_score(y_test,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\Jiwon\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Jiwon\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  5.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로지스틱회귀 최고 C 모수: {'C': 10}\n",
      "TF-IDF Vectorized 로지스틱 회귀의 예측 정확도는 0.704\n"
     ]
    }
   ],
   "source": [
    "#GridSearchCV 써서, 로지스틱 회귀의 하이퍼모수 최적화하자:\n",
    "#(C 모수만 변경하면서, 최적의 C값을 찾기)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params={'C':[0.01,0.1,1,5,10]}\n",
    "grid_cv_lr=GridSearchCV(lr_clf, param_grid=params, cv=3, scoring='accuracy',verbose=1)\n",
    "grid_cv_lr.fit(X_train_tfidf_vect,y_train)\n",
    "print('로지스틱회귀 최고 C 모수:',grid_cv_lr.best_params_)\n",
    "\n",
    "pred=grid_cv_lr.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Vectorized 로지스틱 회귀의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[사이킷런 파이프라인 사용, GridSearchCV와의 결합]\n",
    "- 사이킷런의 Pipeline 클래스: 1) 피처 백터화 + 2) 머신러닝 알고리즘 학습/예측을 한 번에 가능하게 해줌 (시간 단축효과)\n",
    "- 텍스트기반의 피처백터화 등 모든 데이터 전처리 작업, 그리고 이와 Estimator를 결합 가능\n",
    "(ex. 스케일링, 벡터 정규화, PCA 등의 변환작업과 분류, 회귀 등의 Estimator를 한 번에 결합)\n",
    "- 파이프라인 선언하는 방법:\n",
    "pipeline=Pipeline([('tfidf_vect',TfidfVectorizer(stop_words='english')),('lr_clf',LogisticRegression(random_state=156))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이프라인을 통한 로지스틱 회귀의 예측 정확도는 0.704\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pipeline=Pipeline([\n",
    "    ('tfidf_vect',TfidfVectorizer(stop_words='english',ngram_range=(1,2),max_df=300)),\n",
    "    ('lr_clf',LogisticRegression(C=10))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train,y_train)\n",
    "pred=pipeline.predict(X_test)\n",
    "print('파이프라인을 통한 로지스틱 회귀의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사이킷런은 GridSearchCV에 Pipeline을 입력하면서 1) TfidfVectorizer의 모수 + 2) Logistic Regression의 하이퍼 모수를 함께 최적화 가능\n",
    "- GridSearchCV에 (Estimator 말고) Pipeline을 입력할 경우, param_grid의 입력값 설정이 기존과 다름\n",
    ": 딕셔너리 형태의 Key (: 하이퍼 모수명이 객체 변수명과 결합됨), Value\n",
    "(ex. TfdifVectorizer 객체변수인 (tfdif_vect)의 (ngram_range) 모수값을 변화시키면서 최적화하길 원하면,\n",
    "1) 객체 변수명: (tfdif_vect)에 + (언더바 2개:__) + 2) 모수명: (ngrarm_range))\n",
    "- 단점: 너무 오래 걸린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<05. 감성 분석 (Sentiment Analysis)>\n",
    "- 문서의 주관적인 감성, 의견, 감정, 기분을 파악하는 것이 목적\n",
    "- 주관적 단어와 문맥을 기반으로, 감성 수치를 계산\n",
    "- 감성지수 = 1)긍정 감성지수 + 2)부정 감성지수 <- 둘을 합해 긍정 or 부정 감성으로 결정\n",
    "(1) 지도학습 (= 텍스트 기반의 이진분류)\n",
    ": 학습데이터, 타겟(Y)레이블값 기반으로 학습한 뒤, 다른 데이터의 감성분석을 예측\n",
    "(일반적인 텍스트 분류와 유사함)\n",
    "(2) 비지도학습\n",
    ": 감성어휘사전 (Lexicon) 이용하여, 텍스트의 긍정적 or 부정적 감성 여부를 판단\n",
    "\n",
    "- 지도학습 기반의 감성분석 실습 예제 (IMDB 영화평)\n",
    ": 영화평 텍스트를 분석 -> 감성분석 결과가 긍정 or 부정인지 예측하는 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "review_df=pd.read_csv('labeledTrainData.tsv',header=0,sep=\"\\t\",quoting=3)\n",
    "review_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
     ]
    }
   ],
   "source": [
    "print(review_df['review'][0]) #(review 열의, 첫번째[0] 행만 출력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\\\"The Classic War of the Worlds\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \\\"critics\\\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\\"critics\\\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \\\"critics\\\" perceive to be its shortcomings.\"\n"
     ]
    }
   ],
   "source": [
    "print(review_df['review'][1]) #(review 열의, 두번째[1] 행만 출력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "review_df['review']=review_df['review'].str.replace('<br \\>',' ')\n",
    "\n",
    "review_df['review']=review_df['review'].apply(lambda x: re.sub(\"[^a-zA-Z]\",\" \",x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 결정값 클래스인 sentiment열 -> 추출해 (1) 결정값 데이터셋 생성\n",
    "2. 원데이터에서 id, sentiment열 삭제 -> (2) 피처 데이터셋 생성\n",
    "3. train_test_split() -> 학습용, 테스트용 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17500, 1), (7500, 1))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class_df=review_df['sentiment']\n",
    "feature_df=review_df.drop(['id','sentiment'],axis=1,inplace=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(feature_df,class_df,test_size=0.3,random_state=156)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제는, 감상평(리뷰) 텍스트를: 1) 피처 벡터화 -> 2) 머신러닝 분류 알고리즘으로 예측성능 측정\n",
    ": Pipeline 객체를 써서 위 2가지를 한번에 수행\n",
    ": 1) Count 벡터화 적용, 2) TF-IDF 벡터화 적용 -> 예측 성능을 측정\n",
    ": 분류기 = 로지스틱회귀 사용\n",
    ": 예측성능 평가 = 테스트 데이터셋의 정확도와 ROC-AUC 측정 (이진분류 자료이므로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도는 0.8867, ROC-AUC는 0.9505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "#(1) 카운터 벡터화 방법:\n",
    "pipeline=Pipeline([\n",
    "    ('cnt_vect', CountVectorizer(stop_words='english',ngram_range=(1,2))),\n",
    "    ('lr_clf', LogisticRegression(C=10))])\n",
    "\n",
    "pipeline.fit(X_train['review'],y_train)\n",
    "pred=pipeline.predict(X_test['review'])\n",
    "pred_probs=pipeline.predict_proba(X_test['review'])[:,1]\n",
    "\n",
    "print('예측 정확도는 {0:.4f}, ROC-AUC는 {1:.4f}'.format(accuracy_score(y_test,pred), roc_auc_score(y_test, pred_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jiwon\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도는 0.8932, ROC-AUC는 0.9600\n"
     ]
    }
   ],
   "source": [
    "#(2) TF-IDF 벡터화 방법 (성능이 더 우수):\n",
    "pipeline=Pipeline([\n",
    "    ('tfidf_vect',TfidfVectorizer(stop_words='english',ngram_range=(1,2))),\n",
    "    ('lr_clf',LogisticRegression(C=10))])\n",
    "\n",
    "pipeline.fit(X_train['review'],y_train)\n",
    "pred=pipeline.predict(X_test['review'])\n",
    "pred_probs=pipeline.predict_proba(X_test['review'])[:,1]\n",
    "\n",
    "print('예측 정확도는 {0:.4f}, ROC-AUC는 {1:.4f}'.format(accuracy_score(y_test,pred), roc_auc_score(y_test, pred_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[비지도학습 기반 감성분석하기]\n",
    "- Lexicon (어휘집, 감성사전)을 기반으로 하는 것\n",
    "- 이전 지도학습 방법에서는, 데이터에 레이블(Y)값이 있었으나, 현실에서는 드뭄\n",
    "- 감성사전은 1) 긍정 or 2) 부정 감성의 정도를 나타내는 수치 (감성지수(Polarity score))를 포함\n",
    "(감성지수는, 단어의 위치, 주변 단어, 문맥, POS (Part of Speech) 등을 활용해 결정됨)\n",
    "\n",
    "- 대표: NLTK 패키지 <- 많은 서브모듈 있으며, 그 중 Lexicon 모듈을 포함\n",
    "(단점: 성능이 별로임)\n",
    "- NLP 패키지 > WordNet (모듈): 영어 어휘사전, 시멘틱 분석을 제공\n",
    "cf. 시멘틱(Semantic): 문맥상 의미\n",
    ": WordNet은 다양한 상황에서 같은 어휘라도 다르게 사용되는 어휘의 시멘틱 정보를 제공\n",
    ": 각 품사로 구성된 개별단어를 Synset (Sets of cognitive synonyms) 개념을 이용해 표현\n",
    "\n",
    "-주요 감성사전들:\n",
    "\n",
    "(1) SentiWordNet\n",
    ": NLTK 패키지의 WordNet과 비슷 (즉, 감성단어 전용의 WordNet을 구현함)\n",
    ": WordNet의 Synset 개념을 감성분석에 적용한 것\n",
    ": WordNet의 Synset별로 3가지 감성점수(Sentiment score)를 할당함\n",
    "= 1) 긍정 감성지수, 2) 부정 감성지수, 3) 객관성지수 (: 단어가 감성과 관계없이 얼마나 객관적인지 수치화함)\n",
    "-> 문장별로, 단어들의 감성지수를 합해 최종 점수를 계산하고, 이에 기반해 감성이 긍정 vs. 부정 결정\n",
    "\n",
    "(2) VADER\n",
    ": SNS 텍스트에 대한 감성분석용 (성능 우수, 시간 빠름)\n",
    "\n",
    "(3) Pattern\n",
    ": 예측 성능이 아주 우수\n",
    "(단점: 파이썬 2.x 버전에서만 동작)\n",
    "\n",
    "[SentiWordNet을 이용한 감성분석]\n",
    "\n",
    "(1) WordNet Synset과, SentiWordNet SentiSynset 클래스 이해하기\n",
    "- 먼저, WordNet을 이용하려면, NLTK의 모든 데이터셋과 패키지를 다운로드해야:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\Jiwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- WordNet 모듈을 import하여, 'present' 단어에 대한 Synset 추출하기:\n",
    "(WordNet의 synsets()는 모수로 지정된 단어에 대해, WordNet에 있는 모든 Synset 객체를 반환해 줌)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synsets() 반환 타입: <class 'list'>\n",
      "synsets() 반환값 개수: 18\n",
      "synsets() 반환값: [Synset('present.n.01'), Synset('present.n.02'), Synset('present.n.03'), Synset('show.v.01'), Synset('present.v.02'), Synset('stage.v.01'), Synset('present.v.04'), Synset('present.v.05'), Synset('award.v.01'), Synset('give.v.08'), Synset('deliver.v.01'), Synset('introduce.v.01'), Synset('portray.v.04'), Synset('confront.v.03'), Synset('present.v.12'), Synset('salute.v.06'), Synset('present.a.01'), Synset('present.a.02')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "term='present'\n",
    "\n",
    "synsets=wn.synsets(term) #'present'라는 단어로 wordnet의 synsets 생성하기\n",
    "\n",
    "print('synsets() 반환 타입:',type(synsets))\n",
    "print('synsets() 반환값 개수:',len(synsets))\n",
    "print('synsets() 반환값:',synsets) #여러개의 (Synset 객체를 갖는) 리스트를 출력 (총 18개, 서로 다른 문맥을 가짐)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": Synset('present.n.01')\n",
    "- Synset 객체의 모수 'present.n.01'는 POS(품사) 태그를 의미함\n",
    "- 1) present -> 의미\n",
    "2) n -> 품사 (명사)\n",
    "3) 01 -> present가 명사로서 갖는 의미가 여러가지라서 이를 구분한 인덱스\n",
    "\n",
    "cf. synset 객체가 가지는 여러가지 속성:\n",
    "1) POS (Part of Speech, 품사)\n",
    "2) 정의\n",
    "3) 부명제 (Lemma) 등으로 문맥 요소들을 가짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Synset name: present.n.01 #####\n",
      "POS: noun.time\n",
      "Definition: the period of time that is happening now; any continuous stretch of time including the moment of speech\n",
      "Lemmas: ['present', 'nowadays']\n",
      "##### Synset name: present.n.02 #####\n",
      "POS: noun.possession\n",
      "Definition: something presented as a gift\n",
      "Lemmas: ['present']\n",
      "##### Synset name: present.n.03 #####\n",
      "POS: noun.communication\n",
      "Definition: a verb tense that expresses actions or states at the time of speaking\n",
      "Lemmas: ['present', 'present_tense']\n",
      "##### Synset name: show.v.01 #####\n",
      "POS: verb.perception\n",
      "Definition: give an exhibition of to an interested audience\n",
      "Lemmas: ['show', 'demo', 'exhibit', 'present', 'demonstrate']\n",
      "##### Synset name: present.v.02 #####\n",
      "POS: verb.communication\n",
      "Definition: bring forward and present to the mind\n",
      "Lemmas: ['present', 'represent', 'lay_out']\n",
      "##### Synset name: stage.v.01 #####\n",
      "POS: verb.creation\n",
      "Definition: perform (a play), especially on a stage\n",
      "Lemmas: ['stage', 'present', 'represent']\n",
      "##### Synset name: present.v.04 #####\n",
      "POS: verb.possession\n",
      "Definition: hand over formally\n",
      "Lemmas: ['present', 'submit']\n",
      "##### Synset name: present.v.05 #####\n",
      "POS: verb.stative\n",
      "Definition: introduce\n",
      "Lemmas: ['present', 'pose']\n",
      "##### Synset name: award.v.01 #####\n",
      "POS: verb.possession\n",
      "Definition: give, especially as an honor or reward\n",
      "Lemmas: ['award', 'present']\n",
      "##### Synset name: give.v.08 #####\n",
      "POS: verb.possession\n",
      "Definition: give as a present; make a gift of\n",
      "Lemmas: ['give', 'gift', 'present']\n",
      "##### Synset name: deliver.v.01 #####\n",
      "POS: verb.communication\n",
      "Definition: deliver (a speech, oration, or idea)\n",
      "Lemmas: ['deliver', 'present']\n",
      "##### Synset name: introduce.v.01 #####\n",
      "POS: verb.communication\n",
      "Definition: cause to come to know personally\n",
      "Lemmas: ['introduce', 'present', 'acquaint']\n",
      "##### Synset name: portray.v.04 #####\n",
      "POS: verb.creation\n",
      "Definition: represent abstractly, for example in a painting, drawing, or sculpture\n",
      "Lemmas: ['portray', 'present']\n",
      "##### Synset name: confront.v.03 #####\n",
      "POS: verb.communication\n",
      "Definition: present somebody with something, usually to accuse or criticize\n",
      "Lemmas: ['confront', 'face', 'present']\n",
      "##### Synset name: present.v.12 #####\n",
      "POS: verb.communication\n",
      "Definition: formally present a debutante, a representative of a country, etc.\n",
      "Lemmas: ['present']\n",
      "##### Synset name: salute.v.06 #####\n",
      "POS: verb.communication\n",
      "Definition: recognize with a gesture prescribed by a military regulation; assume a prescribed position\n",
      "Lemmas: ['salute', 'present']\n",
      "##### Synset name: present.a.01 #####\n",
      "POS: adj.all\n",
      "Definition: temporal sense; intermediate between past and future; now existing or happening or in consideration\n",
      "Lemmas: ['present']\n",
      "##### Synset name: present.a.02 #####\n",
      "POS: adj.all\n",
      "Definition: being or existing in a specified place\n",
      "Lemmas: ['present']\n"
     ]
    }
   ],
   "source": [
    "for synset in synsets:\n",
    "    print('##### Synset name:',synset.name(), '#####')\n",
    "    print('POS:',synset.lexname())\n",
    "    print('Definition:',synset.definition())\n",
    "    print('Lemmas:',synset.lemma_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": Synset('present.n.01')과 Synset('present.n.02')는 명사인데 서로 다른 의미를 가짐\n",
    "(1) 01번은 \n",
    "1) POS가 noun.time\n",
    "2) Definition이 '시간적인 의미로 현재'\n",
    "(2) 02번은\n",
    "1) POS가 noun.possession\n",
    "2) Definition이 '선물'\n",
    ": 이처럼, synset은 한 단어가 가질 수 있는 여러가지 문맥(시맨틱) 정보를 개별 클래스로 나타냄\n",
    "\n",
    "- WordNet은 어휘간 관계를 유사도로 나타낼 수 있다.\n",
    ": path_similarity() 활용\n",
    "ex. tree, lion, tiger, cat, dog 단어 간에 상호 유사도 예제:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree</th>\n",
       "      <th>lion</th>\n",
       "      <th>tiger</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tree</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lion</th>\n",
       "      <td>0.07</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiger</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tree  lion  tiger   cat   dog\n",
       "tree   1.00  0.07   0.07  0.08  0.12\n",
       "lion   0.07  1.00   0.33  0.25  0.17\n",
       "tiger  0.07  0.33   1.00  0.25  0.17\n",
       "cat    0.08  0.25   0.25  1.00  0.20\n",
       "dog    0.12  0.17   0.17  0.20  1.00"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#synset 객체를 단어별로 생성하기:\n",
    "tree=wn.synset('tree.n.01')\n",
    "lion=wn.synset('lion.n.01')\n",
    "tiger=wn.synset('tiger.n.02')\n",
    "cat=wn.synset('cat.n.01')\n",
    "dog=wn.synset('dog.n.01')\n",
    "\n",
    "entities=[tree, lion, tiger, cat, dog]\n",
    "similarities=[]\n",
    "entity_names=[entity.name().split('.')[0] for entity in entities]\n",
    "\n",
    "#단어별로 synset을 반복하며, 다른 단어의 synset과 유사도를 측정하기:\n",
    "for entity in entities:\n",
    "    similarity=[round(entity.path_similarity(compared_entity),2)\n",
    "               for compared_entity in entities]\n",
    "    similarities.append(similarity)\n",
    "    \n",
    "#개별 단어별 synset과 다른 단어의 synset 간 유사도를 데이터프레임에 저장하기:\n",
    "similarity_df=pd.DataFrame(similarities, columns=entity_names, index=entity_names)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": (해석)\n",
    "lion은 1) tree와 유사도: 0.07 (min), 2) tiger와 유사도: 0.33 (max)\n",
    "\n",
    "- SentiWordNet은 (WordNet의 Synset과 유사한) Senti_Synset 클래스를 가짐\n",
    ": (synsets()와 비슷하게) Senti_Synset 클래스를 리스트 형태로 반환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "senti_synsets() 반환 타입: <class 'list'>\n",
      "senti_synsets() 반환값 개수: 11\n",
      "senti_synsets() 반환값: [SentiSynset('decelerate.v.01'), SentiSynset('slow.v.02'), SentiSynset('slow.v.03'), SentiSynset('slow.a.01'), SentiSynset('slow.a.02'), SentiSynset('dense.s.04'), SentiSynset('slow.a.04'), SentiSynset('boring.s.01'), SentiSynset('dull.s.08'), SentiSynset('slowly.r.01'), SentiSynset('behind.r.03')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "senti_synsets=list(swn.senti_synsets('slow'))\n",
    "print('senti_synsets() 반환 타입:',type(senti_synsets))\n",
    "print('senti_synsets() 반환값 개수:',len(senti_synsets))\n",
    "print('senti_synsets() 반환값:',senti_synsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SentiSynset 객체: 감성지수, 객관성지수 (감성이 없으면 1 (이 때, 감성지수는 모두 0))를 가짐\n",
    "\n",
    "ex. 1) father, 2) fabulous(아주 멋진)에 대해 감성지수, 객관성지수 알아보기:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "father 긍정감성지수: 0.0\n",
      "father 부정감성지수: 0.0\n",
      "father 객관성지수: 1.0\n",
      "\n",
      "\n",
      "fabulous 긍정감성지수: 0.875\n",
      "fabulous 부정감성지수: 0.125\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "father=swn.senti_synset('father.n.01')\n",
    "print('father 긍정감성지수:',father.pos_score())\n",
    "print('father 부정감성지수:',father.neg_score())\n",
    "print('father 객관성지수:',father.obj_score())\n",
    "print('\\n')\n",
    "\n",
    "fabulous=swn.senti_synset('fabulous.a.01')\n",
    "print('fabulous 긍정감성지수:',fabulous.pos_score())\n",
    "print('fabulous 부정감성지수:',fabulous.neg_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SentiWordNet 이용해 영화 감상평 감성분석하기]\n",
    "- SentiWordNet Lexicon 기반으로 감성분석 수행 절차:\n",
    "(1) 문서(document) -> 문장(sentence)로 분해\n",
    "(2) 문장 -> 단어(word)로 토큰화 & 품사 태깅\n",
    "(3) Senti_synset에서 긍정/부정 감성지수 계산 후, 합산 값이 특정값 이상이면 긍정감성으로 결정\n",
    "\n",
    "- SentiWordNet 이용하기 위해서,\n",
    "(WordNet 이용해 문서 -> 단어로 토큰화)한 후,\n",
    "1) 어근추출(Lemmatization), 2) 품사태깅(POS Tagging)을 적용함.\n",
    "\n",
    "먼저, 품사태깅하는 사용자 정의함수 만들기:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "#간단한 NTLK PennTreebank Tag 기반으로, WordNet 기반의 품사 태그로 변환시키기:\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'): return wn.ADJ\n",
    "    elif tag.startswith('N'): return wn.NOUN\n",
    "    elif tag.startswith('R'): return wn.ADV\n",
    "    elif tag.startswith('V'): return wn.VERB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제, 문서를 (문장 -> 단어 토큰 -> 품사 태깅한 뒤) SentiSynset 클래스를 생성하고, Polarity Score를 합산하는 함수 만들기:\n",
    "(긍정+부정 감성지수 합한 총 감성지수가 0이상이면 '긍정'감성, 아니면 '부정'감성으로 예측함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "\n",
    "def swn_polarity(text):\n",
    "    #감성지수 초기화\n",
    "    sentiment=0.0\n",
    "    tokens_count=0\n",
    "    \n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    raw_sentences=sent_tokenize(text)\n",
    "    \n",
    "    #분해된 문장별로 단어 토큰 -> 품사태깅 후, SentiSynset 생성 -> 감성지수 합산\n",
    "    for raw_sentence in raw_sentences:\n",
    "        #NLTK 기반의 품사 태깅 문장 추출\n",
    "        tagged_sentence=pos_tag(word_tokenize(raw_sentence))\n",
    "        for word, tag in tagged_sentence:\n",
    "            #WordNet 기반 품사태깅과 어근추출\n",
    "            wn_tag=penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "                continue\n",
    "            lemma=lemmatizer.lemmatize(word,pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    "                \n",
    "            #어근을 추출한 단어와, WordNet 기반 품사태깅을 입력해, Synset 객체 만들기:\n",
    "            synsets=wn.synsets(lemma, pos=wn_tag)\n",
    "            if not synsets:\n",
    "                continue\n",
    "                \n",
    "            #sentiwordnet의 감성단어 분석으로, 감성 synset 추출하기:\n",
    "            #모든 단어에 대해 긍정감성지수는 (+)으로, 부정감성지수는 (-)으로 합산해 감성지수 계산하기:\n",
    "            synset=synsets[0]\n",
    "            swn_synset=swn.senti_synset(synset.name())\n",
    "            sentiment+=(swn_synset.pos_score()-swn_synset.neg_score())\n",
    "            tokens_count+=1\n",
    "            \n",
    "    if not tokens_count: return 0\n",
    "    \n",
    "    #총 점수가 0이상이면 긍정(1), 아니면 부정(0)을 반환하기:\n",
    "    if sentiment>=0: return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score , recall_score , confusion_matrix\n",
    "\n",
    "def get_clf_eval(y_test , pred):\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    precision = precision_score(y_test , pred)\n",
    "    recall = recall_score(y_test , pred)\n",
    "    print('오차 행렬')\n",
    "    print(confusion)\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}'.format(accuracy , precision ,recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df['preds']=review_df['review'].apply(lambda x: swn_polarity(x))\n",
    "y_target=review_df['sentiment'].values\n",
    "preds=review_df['preds'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### SentiWordNet 예측성능 평가 #####\n",
      "오차 행렬\n",
      "[[7669 4831]\n",
      " [3644 8856]]\n",
      "정확도: 0.6610, 정밀도: 0.6470, 재현율: 0.7085\n"
     ]
    }
   ],
   "source": [
    "print('##### SentiWordNet 예측성능 평가 #####')\n",
    "get_clf_eval(y_target,preds) #만족하기 어려운 수치임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[VADER를 이용한 감성 분석]\n",
    "- 또 다른 VADER Lexicon으로 감성분석하기 (SNS 텍스트에 대해 활용도가 높으며, 룰 기반의 렉스콘임)\n",
    "- SentimentIntensityAnalyzer 클래스 이용\n",
    "- VADER는, 1) NLTK 패키지의 서브모듈로 제공 (이 방법 사용)\n",
    "or 2) 단독 패키지로 제공\n",
    "\n",
    "- VADER 사용법:\n",
    "NLTK 서브모듈로 (SentimentIntensityAnalyzer를 import)하고, IMDB의 감상평 한 개만 감성분석 하기 (아래 예제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.127, 'neu': 0.747, 'pos': 0.125, 'compound': -0.7943}\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "senti_analyzer=SentimentIntensityAnalyzer()\n",
    "senti_scores=senti_analyzer.polarity_scores(review_df['review'][0])\n",
    "print(senti_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VADER 쓰면, 쉽게 감성분석이 가능:\n",
    "1) 먼저 SentimentIntensityAnalyzer 객체를 생성\n",
    "-> 2) 문서별로 polarity_scores() 매서드를 호출 (: 감성점수 계산됨)\n",
    "-> 3) 그 문서의 감성점수가, 특정 임계값 이상이면 (neg), 아니면 (pos)으로 판단내림 (cf. neu: 중립적인 감성지수)\n",
    "cf. compound: 3가지 scores를 조합하여 -1에서 1 사이의 감성지수 값으로 합친 값\n",
    "- compound score를 기반으로, (+) or (-)를 결정함\n",
    "(보통 0.1 이상 = 긍정, 그 이하 = 부정 (또는, 임계값을 직접 지정하여 예측성능을 향상 가능함))\n",
    "\n",
    "- VADER를 이용하여, IMDB의 감성분석 수행하기:\n",
    "1) vader_polarity()함수 새로 만들기\n",
    "(: 입력모수 = 텍스트, 임계값을 가짐),\n",
    "2) SentimentIntensityAnalyzer 객체의 polarity_scores() 메서드를 호출해 감성결과를 반환함\n",
    "\n",
    "- review_df의 데이터프레임의 apply lambda식을 통해, vader_polarity()함수 호출해, 각 문서별 감성결과를 'vader_preds'라는 \n",
    "review_df의 새 열 만들어 저장하고, 저장된 감성분석 결과에 근거해 VADER의 예측성능을 측정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_polarity(review,threshold=0.1):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    \n",
    "    # compound 값에 기반하여 threshold 입력값보다 크면 1, 그렇지 않으면 0을 반환 \n",
    "    agg_score = scores['compound']\n",
    "    final_sentiment = 1 if agg_score >= threshold else 0\n",
    "    return final_sentiment\n",
    "\n",
    "# apply lambda 식을 이용하여 레코드별로 vader_polarity( )를 수행하고 결과를 'vader_preds'에 저장\n",
    "review_df['vader_preds'] = review_df['review'].apply( lambda x : vader_polarity(x, 0.1) )\n",
    "y_target = review_df['sentiment'].values\n",
    "vader_preds = review_df['vader_preds'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### VADER 예측 성능 평가 ####\n",
      "[[ 6736  5764]\n",
      " [ 1866 10634]]\n",
      "정확도: 0.6948\n",
      "정밀도: 0.6484937187461886\n",
      "재현율: 0.85072\n"
     ]
    }
   ],
   "source": [
    "print('#### VADER 예측 성능 평가 ####')\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score \n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
    "\n",
    "print(confusion_matrix( y_target, vader_preds))\n",
    "print(\"정확도:\", accuracy_score(y_target , vader_preds))\n",
    "print(\"정밀도:\", precision_score(y_target , vader_preds))\n",
    "print(\"재현율:\", recall_score(y_target, vader_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
